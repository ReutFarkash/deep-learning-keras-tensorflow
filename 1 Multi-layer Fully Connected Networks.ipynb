{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/keras-logo-small.jpg\" width=\"20%\" />\n",
    "\n",
    "## Keras: The Deep Learning library _for perfectionist, with deadlines_ $^1$\n",
    "\n",
    "_`[1]`: Freely borrowed by the [Django](https://www.djangoproject.com) payoff_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**excerpt** from the officila documentation ([http://keras.io](http://keras.io))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Keras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/cntk), or [Theano](https://github.com/Theano/Theano). It was developed with a focus on enabling fast experimentation. \n",
    ">\n",
    "> _Being able to go from idea to result with the least possible delay is key to doing good research._\n",
    ">\n",
    "> Use Keras if you need a deep learning library that:\n",
    "> \n",
    "> * Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "> * Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "> * Runs seamlessly on CPU and GPU.\n",
    ">\n",
    ">\n",
    "> Keras is compatible with: **Python 2.7-3.5**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"kaggle\"></a>\n",
    "### Kaggle Challenge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The Otto Group is one of the worldâ€™s biggest e-commerce companies, A consistent analysis of the performance of products is crucial. \n",
    ">\n",
    ">However, due to diverse global infrastructure, many identical products get classified differently.\n",
    ">\n",
    "> For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories.\n",
    ">\n",
    ">Each row corresponds to a single product. There are a total of 93 numerical features, which represent counts of different events. All features have been obfuscated and will not be defined any further.\n",
    "\n",
    "https://www.kaggle.com/c/otto-group-product-classification-challenge/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For this section we will use the Kaggle Otto Group Challenge Data. You will find these data in \n",
    "`data/kaggle_ottogroup/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This algorithm has nothing to do with the canonical _linear regression_, but it is an algorithm that allows us to solve problems of classification (supervised learning). \n",
    "\n",
    "In fact, to estimate the dependent variable, now we make use of the so-called **logistic function** or **sigmoid**. \n",
    "\n",
    "It is precisely because of this feature we call this algorithm logistic regression.\n",
    "\n",
    "![](imgs/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reut/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from kaggle_data import load_data, preprocess_data, preprocess_labels\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 classes\n",
      "93 features\n"
     ]
    }
   ],
   "source": [
    "X_train, labels = load_data('data/kaggle_ottogroup/train.csv', train=True)\n",
    "X_train, scaler = preprocess_data(X_train)\n",
    "Y_train, encoder = preprocess_labels(labels)\n",
    "\n",
    "X_test, ids = load_data('data/kaggle_ottogroup/test.csv', train=False)\n",
    "X_test, _ = preprocess_data(X_test, scaler)\n",
    "\n",
    "nb_classes = Y_train.shape[1]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6',\n",
       "       'Class_7', 'Class_8', 'Class_9'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train  # one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import theano as th\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Based on example from DeepLearning.net\n",
    "rng = np.random\n",
    "N = 400\n",
    "feats = 93\n",
    "training_steps = 10\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w = th.shared(rng.randn(feats), name=\"w\")\n",
    "b = th.shared(0., name=\"b\")\n",
    "\n",
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))         # Probability that target = 1\n",
    "prediction = p_1 > 0.5                          # The prediction thresholded\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)   # Cross-entropy loss function\n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum()      # The cost to minimize\n",
    "gw, gb = T.grad(cost, [w, b])                   # Compute the gradient of the cost\n",
    "                                                \n",
    "\n",
    "# Compile\n",
    "train = th.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)),\n",
    "          allow_input_downcast=True)\n",
    "\n",
    "predict = th.function(inputs=[x], outputs=prediction, allow_input_downcast=True)\n",
    "\n",
    "#Transform for class1\n",
    "y_class1 = []\n",
    "for i in Y_train:\n",
    "    y_class1.append(i[0])\n",
    "y_class1 = np.array(y_class1)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    print('Epoch %s' % (i+1,))\n",
    "    pred, err = train(X_train, y_class1)\n",
    "\n",
    "print(\"target values for Data:\")\n",
    "print(y_class1)\n",
    "print(\"prediction on training set:\")\n",
    "print(predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(\"float\", [None, dims]) \n",
    "y = tf.placeholder(\"float\", [None, nb_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(?, 93) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (Introducing Tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct (linear) model\n",
    "with tf.name_scope(\"model\") as scope:\n",
    "    # Set model weights\n",
    "    W = tf.Variable(tf.zeros([dims, nb_classes]))\n",
    "    b = tf.Variable(tf.zeros([nb_classes]))\n",
    "    activation = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "    # Add summary ops to collect data\n",
    "    w_h = tf.summary.histogram(\"weights_histogram\", W)\n",
    "    b_h = tf.summary.histogram(\"biases_histograms\", b)\n",
    "    tf.summary.scalar('mean_weights', tf.reduce_mean(W))\n",
    "    tf.summary.scalar('mean_bias', tf.reduce_mean(b))\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "# Note: More name scopes will clean up graph representation\n",
    "with tf.name_scope(\"cost_function\") as scope:\n",
    "    cross_entropy = y*tf.log(activation)\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(cross_entropy,reduction_indices=1))\n",
    "    # Create a summary to monitor the cost function\n",
    "    tf.summary.scalar(\"cost_function\", cost)\n",
    "    tf.summary.histogram(\"cost_histogram\", cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    # Set the Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Accuracy') as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # Create a summary to monitor the cost function\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in a TF Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reut\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = \"/tmp/logistic_logs\"\n",
    "import os, shutil\n",
    "if os.path.isdir(LOGDIR):\n",
    "    print(\"reut\")\n",
    "    shutil.rmtree(LOGDIR)\n",
    "os.mkdir(LOGDIR)\n",
    "\n",
    "# Plug TensorBoard Visualisation \n",
    "writer = tf.summary.FileWriter(LOGDIR, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/weights_histogram:0\n",
      "model/biases_histograms:0\n",
      "model/mean_weights:0\n",
      "model/mean_bias:0\n",
      "cost_function/cost_function:0\n",
      "cost_function/cost_histogram:0\n",
      "Accuracy/accuracy:0\n",
      "Tensor(\"add:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for var in tf.get_collection(tf.GraphKeys.SUMMARIES):\n",
    "    print(var.name)\n",
    "    \n",
    "summary_op = tf.summary.merge_all()\n",
    "print('Summary Op: ' + summary_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy epoch 0:0.6649535894393921\n",
      "accuracy epoch 1:0.665276825428009\n",
      "accuracy epoch 2:0.6657131910324097\n",
      "accuracy epoch 3:0.6659556031227112\n",
      "accuracy epoch 4:0.6662949919700623\n",
      "accuracy epoch 5:0.6666020154953003\n",
      "accuracy epoch 6:0.6668121218681335\n",
      "accuracy epoch 7:0.6671029925346375\n",
      "accuracy epoch 8:0.6674585342407227\n",
      "accuracy epoch 9:0.667830228805542\n",
      "accuracy epoch 10:0.6680887937545776\n",
      "accuracy epoch 11:0.6682342886924744\n",
      "accuracy epoch 12:0.6684605479240417\n",
      "accuracy epoch 13:0.6687514185905457\n",
      "accuracy epoch 14:0.6690422892570496\n",
      "accuracy epoch 15:0.6692523956298828\n",
      "accuracy epoch 16:0.6695109605789185\n",
      "accuracy epoch 17:0.6697695255279541\n",
      "accuracy epoch 18:0.6699796319007874\n",
      "accuracy epoch 19:0.6702058911323547\n",
      "accuracy epoch 20:0.6705452799797058\n",
      "accuracy epoch 21:0.6708361506462097\n",
      "accuracy epoch 22:0.6710785627365112\n",
      "accuracy epoch 23:0.671385645866394\n",
      "accuracy epoch 24:0.6716926693916321\n",
      "Training phase finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f706cc3f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 5 ... 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    # Initializing the variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    cost_epochs = []\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        _, summary, c = session.run(fetches=[optimizer, summary_op, cost], \n",
    "                                    feed_dict={x: X_train, y: Y_train})\n",
    "        cost_epochs.append(c)\n",
    "        writer.add_summary(summary=summary, global_step=epoch)\n",
    "        print(\"accuracy epoch {}:{}\".format(epoch, accuracy.eval({x: X_train, y: Y_train})))\n",
    "        \n",
    "    print(\"Training phase finished\")\n",
    "    \n",
    "    #plotting\n",
    "    plt.plot(range(len(cost_epochs)), cost_epochs, 'o', label='Logistic Regression Training phase')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    prediction = tf.argmax(activation, 1)\n",
    "    print(prediction.eval({x: X_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#python -m tensorflow.tensorboard --logdir=/tmp/logistic_logs\n",
    "tensorboard --logdir='/tmp/logistic_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 dims\n",
      "Building model...\n",
      "9 classes\n",
      "Epoch 1/1\n",
      "61878/61878 [==============================] - 3s 51us/step - loss: 2.0051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70206e79b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = X_train.shape[1]\n",
    "print(dims, 'dims')\n",
    "print(\"Building model...\")\n",
    "\n",
    "nb_classes = Y_train.shape[1]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nb_classes, input_shape=(dims,), activation='sigmoid'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplicity is pretty impressive, right? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets understand:\n",
    "- The core data structure of Keras is a <b>model</b>, a way to organize layers. \n",
    "- The main type of model is the <b>Sequential</b> model, a linear stack of layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did here is stacking a Fully Connected (<b>Dense</b>) layer of trainable weights from the input to the output and an <b>Activation</b> layer on top of the weights layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "Dense(units, activation=None, use_bias=True, \n",
    "      kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "      kernel_regularizer=None, bias_regularizer=None, \n",
    "      activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `units`: int > 0.\n",
    "\n",
    "* `kernel_initializer`: Initializer for the kernel weights matrix.\n",
    "\n",
    "* `bias_initializer`: Initializer for the bias vector.\n",
    "\n",
    "* `activation`: name of activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: `a(x) = x`).\n",
    "\n",
    "* `weights`: list of Numpy arrays to set as initial weights. The list should have 2 elements, of shape (input_dim, output_dim) and (output_dim,) for weights and biases respectively.\n",
    "\n",
    "* `kernel_regularizer`: Regulariser function (eg. L1 or L2 regularization), applied to the `kernel` weights matrix.\n",
    "\n",
    "* `bias_regularizer`: Regulariser function applied to the bias vector.\n",
    "\n",
    "* `activity_regularizer`: Regularizer function applied to the output of the layer (its \"activation\").\n",
    "\n",
    "* `kernel_constraint`: Constraint function applied to the main weights matrix.\n",
    "\n",
    "* `bias_constraint`: Constraint function applied to the bias.\n",
    "\n",
    "* `use_bias`: whether to include a bias (i.e. make the layer affine rather than linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output Shapes\n",
    "\n",
    "##### Input shape\n",
    "\n",
    "`nD` tensor with shape: `(batch_size, ..., input_dim)`. The most common situation would be a 2D input with shape `(batch_size, input_dim)`.\n",
    "\n",
    "##### Output shape\n",
    "\n",
    "`nD` tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D input with shape `(batch_size, input_dim)`, the output would have shape `(batch_size, units)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about `*_initializers`\n",
    "\n",
    "Weight initialization can critically affect the speed at which a neural network is able to learn. Under certain circumstances, a poor initialization of weights can prevent a neural network from learning anything.\n",
    "\n",
    "For further details, please refer to this super interesting article from NVIDIA DIGITS: [Weights Initialisers](https://github.com/NVIDIA/DIGITS/blob/master/examples/weight-init/README.md)\n",
    "\n",
    "Keras supports different initialisation strategies: [`keras.initializers`](https://keras.io/initializers/).\n",
    "\n",
    "\n",
    "#### Some Recommended Papers:\n",
    "\n",
    "[1] Bengio, Yoshua. [\"Practical recommendations for gradient-based training of deep architectures.\"](http://arxiv.org/abs/1206.5533) Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012. 437-478.\n",
    "\n",
    "[2] LeCun, Y., Bottou, L., Orr, G. B., and Muller, K. (1998a). [Efficient backprop. In Neural Networks](https://scholar.google.com/scholar?cluster=1366432119558306638&hl=en&as_sdt=0,22), Tricks of the Trade.\n",
    "\n",
    "[3] Glorot, Xavier, and Yoshua Bengio. [\"Understanding the difficulty of training deep feedforward neural networks.\"](https://scholar.google.com/scholar?cluster=17889055433985220047&hl=en&as_sdt=0,22) International conference on artificial intelligence and statistics. 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (some) others `keras.core.layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `keras.layers.core.Flatten()`\n",
    "* `keras.layers.core.Reshape(target_shape)`\n",
    "* `keras.layers.core.Permute(dims)`\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 1), input_shape=(10, 64)))\n",
    "# now: model.output_shape == (None, 64, 10)\n",
    "# note: `None` is the batch dimension\n",
    "```\n",
    "\n",
    "* `keras.layers.core.Lambda(function, output_shape=None, arguments=None)`\n",
    "* `keras.layers.core.ActivityRegularization(l1=0.0, l2=0.0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/dl_overview.png\" >\n",
    "\n",
    "Credits: Yam Peleg ([@Yampeleg](https://twitter.com/yampeleg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "from keras.layers.core import Activation\n",
    "\n",
    "Activation(activation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supported Activations** : [https://keras.io/activations/]\n",
    "\n",
    "**Advanced Activations**: [https://keras.io/layers/advanced-activations/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n",
    "Here we used <b>SGD</b> (stochastic gradient descent) as an optimization algorithm for our trainable weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif\" width=\"40%\">\n",
    "\n",
    "Source & Reference: http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Data Sciencing\" this example a little bit more\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did here is nice, however in the real world it is not useable because of overfitting.\n",
    "Lets try and solve it with cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In overfitting, a statistical model describes random error or noise instead of the underlying relationship. Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. \n",
    "\n",
    "A model that has been overfit has poor predictive performance, as it overreacts to minor fluctuations in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/overfitting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>To avoid overfitting, we will first split out data to training set and test set and test out model on the test set.\n",
    "Next: we will use two of keras's callbacks <b>EarlyStopping</b> and <b>ModelCheckpoint</b></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see first the model we implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 9)                 846       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 846\n",
      "Trainable params: 846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52596 samples, validate on 9282 samples\n",
      "Epoch 1/50\n",
      "52596/52596 [==============================] - 1s 21us/step - loss: 1.8789 - val_loss: 1.8676\n",
      "Epoch 2/50\n",
      "52596/52596 [==============================] - 1s 16us/step - loss: 1.8585 - val_loss: 1.8492\n",
      "Epoch 3/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.8416 - val_loss: 1.8336\n",
      "Epoch 4/50\n",
      "52596/52596 [==============================] - 1s 14us/step - loss: 1.8272 - val_loss: 1.8201\n",
      "Epoch 5/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.8146 - val_loss: 1.8083\n",
      "Epoch 6/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.8034 - val_loss: 1.7977\n",
      "Epoch 7/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7933 - val_loss: 1.7881\n",
      "Epoch 8/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7842 - val_loss: 1.7794\n",
      "Epoch 9/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7759 - val_loss: 1.7714\n",
      "Epoch 10/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7682 - val_loss: 1.7640\n",
      "Epoch 11/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7611 - val_loss: 1.7572\n",
      "Epoch 12/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7544 - val_loss: 1.7508\n",
      "Epoch 13/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7482 - val_loss: 1.7448\n",
      "Epoch 14/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7424 - val_loss: 1.7392\n",
      "Epoch 15/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7370 - val_loss: 1.7340\n",
      "Epoch 16/50\n",
      "52596/52596 [==============================] - 1s 16us/step - loss: 1.7319 - val_loss: 1.7290\n",
      "Epoch 17/50\n",
      "52596/52596 [==============================] - 1s 16us/step - loss: 1.7270 - val_loss: 1.7243\n",
      "Epoch 18/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7225 - val_loss: 1.7199\n",
      "Epoch 19/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7182 - val_loss: 1.7157\n",
      "Epoch 20/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7141 - val_loss: 1.7117\n",
      "Epoch 21/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7102 - val_loss: 1.7080\n",
      "Epoch 22/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7065 - val_loss: 1.7044\n",
      "Epoch 23/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.7030 - val_loss: 1.7009\n",
      "Epoch 24/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6997 - val_loss: 1.6976\n",
      "Epoch 25/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6964 - val_loss: 1.6945\n",
      "Epoch 26/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6934 - val_loss: 1.6915\n",
      "Epoch 27/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6904 - val_loss: 1.6886\n",
      "Epoch 28/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6876 - val_loss: 1.6858\n",
      "Epoch 29/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6849 - val_loss: 1.6831\n",
      "Epoch 30/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6823 - val_loss: 1.6805\n",
      "Epoch 31/50\n",
      "52596/52596 [==============================] - 1s 16us/step - loss: 1.6797 - val_loss: 1.6780\n",
      "Epoch 32/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6773 - val_loss: 1.6756\n",
      "Epoch 33/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6749 - val_loss: 1.6733\n",
      "Epoch 34/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6726 - val_loss: 1.6710\n",
      "Epoch 35/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6704 - val_loss: 1.6689\n",
      "Epoch 36/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6683 - val_loss: 1.6668\n",
      "Epoch 37/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6662 - val_loss: 1.6647\n",
      "Epoch 38/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6642 - val_loss: 1.6627\n",
      "Epoch 39/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6623 - val_loss: 1.6608\n",
      "Epoch 40/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6605 - val_loss: 1.6590\n",
      "Epoch 41/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6587 - val_loss: 1.6572\n",
      "Epoch 42/50\n",
      "52596/52596 [==============================] - 1s 17us/step - loss: 1.6569 - val_loss: 1.6555\n",
      "Epoch 43/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6553 - val_loss: 1.6538\n",
      "Epoch 44/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6536 - val_loss: 1.6522\n",
      "Epoch 45/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6521 - val_loss: 1.6507\n",
      "Epoch 46/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6506 - val_loss: 1.6492\n",
      "Epoch 47/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6491 - val_loss: 1.6477\n",
      "Epoch 48/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6477 - val_loss: 1.6463\n",
      "Epoch 49/50\n",
      "52596/52596 [==============================] - 1s 15us/step - loss: 1.6464 - val_loss: 1.6450\n",
      "Epoch 50/50\n",
      "52596/52596 [==============================] - 1s 16us/step - loss: 1.6450 - val_loss: 1.6437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70180d0d68>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "fBestModel = 'best_model.h5' \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) \n",
    "best_model = ModelCheckpoint(fBestModel, verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=50, \n",
    "          batch_size=128, verbose=True, callbacks=[best_model, early_stop]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Fully Connected Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/MLP.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/backprop.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** _How hard can it be to build a Multi-Layer Fully-Connected Network with keras?_\n",
    "\n",
    "**A:** _It is basically the same, just add more layers!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 100)               9400      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 909       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 10,309\n",
      "Trainable params: 10,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(dims,)))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52596 samples, validate on 9282 samples\n",
      "Epoch 1/20\n",
      "52596/52596 [==============================] - 1s 21us/step - loss: 1.2028 - val_loss: 0.8682\n",
      "Epoch 2/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.8123 - val_loss: 0.7720\n",
      "Epoch 3/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.7529 - val_loss: 0.7339\n",
      "Epoch 4/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.7244 - val_loss: 0.7142\n",
      "Epoch 5/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.7067 - val_loss: 0.6997\n",
      "Epoch 6/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6948 - val_loss: 0.6909\n",
      "Epoch 7/20\n",
      "52596/52596 [==============================] - 1s 17us/step - loss: 0.6860 - val_loss: 0.6838\n",
      "Epoch 8/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6793 - val_loss: 0.6782\n",
      "Epoch 9/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6741 - val_loss: 0.6741\n",
      "Epoch 10/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6697 - val_loss: 0.6706\n",
      "Epoch 11/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6659 - val_loss: 0.6675\n",
      "Epoch 12/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6630 - val_loss: 0.6656\n",
      "Epoch 13/20\n",
      "52596/52596 [==============================] - 1s 17us/step - loss: 0.6605 - val_loss: 0.6633\n",
      "Epoch 14/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6582 - val_loss: 0.6609\n",
      "Epoch 15/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6561 - val_loss: 0.6607\n",
      "Epoch 16/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6544 - val_loss: 0.6588\n",
      "Epoch 17/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6527 - val_loss: 0.6576\n",
      "Epoch 18/20\n",
      "52596/52596 [==============================] - 1s 17us/step - loss: 0.6512 - val_loss: 0.6563\n",
      "Epoch 19/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6500 - val_loss: 0.6546\n",
      "Epoch 20/20\n",
      "52596/52596 [==============================] - 1s 18us/step - loss: 0.6488 - val_loss: 0.6541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70180f8f28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=20, \n",
    "          batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On - Keras Fully Connected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take couple of minutes and try to play with the number of layers and the number of parameters in the layers to get the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               9400      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 279       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 26,359\n",
      "Trainable params: 26,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(dims,)))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(30))\n",
    "\n",
    "# ...\n",
    "# ...\n",
    "# Play with it! add as much layers as you want! try and get better results.\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52596 samples, validate on 9282 samples\n",
      "Epoch 1/70\n",
      "52596/52596 [==============================] - 1s 28us/step - loss: 0.9855 - val_loss: 0.7652\n",
      "Epoch 2/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.7400 - val_loss: 0.7134\n",
      "Epoch 3/70\n",
      "52596/52596 [==============================] - 1s 26us/step - loss: 0.7040 - val_loss: 0.6928\n",
      "Epoch 4/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6871 - val_loss: 0.6794\n",
      "Epoch 5/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6757 - val_loss: 0.6723\n",
      "Epoch 6/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6686 - val_loss: 0.6690\n",
      "Epoch 7/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6627 - val_loss: 0.6647\n",
      "Epoch 8/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6592 - val_loss: 0.6672\n",
      "Epoch 9/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6568 - val_loss: 0.6580\n",
      "Epoch 10/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6540 - val_loss: 0.6598\n",
      "Epoch 11/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6517 - val_loss: 0.6618\n",
      "Epoch 12/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6499 - val_loss: 0.6540\n",
      "Epoch 13/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6494 - val_loss: 0.6522\n",
      "Epoch 14/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6474 - val_loss: 0.6533\n",
      "Epoch 15/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6466 - val_loss: 0.6508\n",
      "Epoch 16/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6456 - val_loss: 0.6512\n",
      "Epoch 17/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6447 - val_loss: 0.6496\n",
      "Epoch 18/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6435 - val_loss: 0.6485\n",
      "Epoch 19/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6430 - val_loss: 0.6469\n",
      "Epoch 20/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6418 - val_loss: 0.6458\n",
      "Epoch 21/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6411 - val_loss: 0.6489\n",
      "Epoch 22/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6412 - val_loss: 0.6444\n",
      "Epoch 23/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6398 - val_loss: 0.6455\n",
      "Epoch 24/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6395 - val_loss: 0.6435\n",
      "Epoch 25/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6393 - val_loss: 0.6429\n",
      "Epoch 26/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6384 - val_loss: 0.6462\n",
      "Epoch 27/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6386 - val_loss: 0.6461\n",
      "Epoch 28/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6373 - val_loss: 0.6431\n",
      "Epoch 29/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6371 - val_loss: 0.6420\n",
      "Epoch 30/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6371 - val_loss: 0.6467\n",
      "Epoch 31/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6365 - val_loss: 0.6423\n",
      "Epoch 32/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6362 - val_loss: 0.6452\n",
      "Epoch 33/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6364 - val_loss: 0.6396\n",
      "Epoch 34/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6359 - val_loss: 0.6419\n",
      "Epoch 35/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6352 - val_loss: 0.6402\n",
      "Epoch 36/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6357 - val_loss: 0.6410\n",
      "Epoch 37/70\n",
      "52596/52596 [==============================] - 1s 24us/step - loss: 0.6354 - val_loss: 0.6398\n",
      "Epoch 38/70\n",
      "52596/52596 [==============================] - 1s 25us/step - loss: 0.6349 - val_loss: 0.6415\n",
      "Epoch 00038: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f701009beb8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fBestModel = 'best_model_reut.h5' \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1) \n",
    "best_model = ModelCheckpoint(fBestModel, verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=70, \n",
    "          batch_size=128, verbose=True, callbacks=[best_model, early_stop]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a question answering system, an image classification model, a Neural Turing Machine, a word2vec embedder or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Motivations for depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Much has been studied about the depth of neural nets. Is has been proven mathematically[1] and empirically that convolutional neural network benifit from depth! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] - On the Expressive Power of Deep Learning: A Tensor Analysis - Cohen, et al 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Motivations for depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One much quoted theorem about neural network states that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Universal approximation theorem states[1] that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] - Approximation Capabilities of Multilayer Feedforward Networks - Kurt Hornik 1991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum\n",
    "\n",
    "[1.1 Keras Backend](1.1  Keras Backend.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
